{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf857b3d-7423-4674-b41c-83f31bc30615",
   "metadata": {},
   "source": [
    "# NLP Walkthrough\n",
    "An example of NLP, using the BBC news data ([which is available here](http://mlg.ucd.ie/datasets/bbc.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f86a91-d50b-4682-a606-79e9550cc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4d716f-4ca2-4c4c-a999-bf35128cc266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225 articles loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the files\n",
    "parent_folder = \"C:\\\\Users\\\\User\\\\Documents\\\\Projects\\\\BBC_News_NLP\\\\Data\\\\bbc\"\n",
    "\n",
    "category_folders = []\n",
    "for folder in os.listdir(parent_folder):\n",
    "    path = os.path.join(parent_folder, folder)\n",
    "\n",
    "    if os.path.isdir(path):\n",
    "        category_folders.append(path)\n",
    "\n",
    "articles_list = []\n",
    "for category in category_folders:\n",
    "    for article in os.listdir(category):\n",
    "        if article.endswith(\".txt\") and not article.startswith(\"README\"):\n",
    "            article_path = os.path.join(category, article)\n",
    "            try: \n",
    "                with open(article_path, \"r\") as file:\n",
    "                    article_text = file.read()\n",
    "                    articles_list.append({\"text\" : article_text, \"category\" : os.path.relpath(category, parent_folder)})\n",
    "            except: \n",
    "                print(f\"Could not load {article_path}\")\n",
    "\n",
    "articles = pd.DataFrame(articles_list)\n",
    "\n",
    "print(f\"{len(articles)} articles loaded\")\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e82b08-f99d-4800-9302-7167dd2a9109",
   "metadata": {},
   "source": [
    "### Process the files for analysis\n",
    "Using tokenisation, lemmatising, stemming, and by removing stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b78efb67-f8ca-4190-9758-347c34b7867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenisation:\n",
      "['Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.', 'The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales.', 'TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.']\n",
      "\n",
      "Word tokenisation:\n",
      "['Ad', 'sales', 'boost', 'Time', 'Warner', 'profit', 'Quarterly', 'profits', 'at', 'US', 'media', 'giant', 'TimeWarner', 'jumped', '76', '%', 'to', '$', '1.13bn', '(', 'Â£600m', ')', 'for', 'the', 'three', 'months', 'to', 'December', ',', 'from', '$', '639m', 'year-earlier', '.'] \n",
      "\n",
      "['The', 'firm', ',', 'which', 'is', 'now', 'one', 'of', 'the', 'biggest', 'investors', 'in', 'Google', ',', 'benefited', 'from', 'sales', 'of', 'high-speed', 'internet', 'connections', 'and', 'higher', 'advert', 'sales', '.'] \n",
      "\n",
      "['TimeWarner', 'said', 'fourth', 'quarter', 'sales', 'rose', '2', '%', 'to', '$', '11.1bn', 'from', '$', '10.9bn', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenisation:\n",
    "first_article = articles[\"text\"][0]\n",
    "\n",
    "first_article_sentences = sent_tokenize(articles[\"text\"][0])\n",
    "print(\"Sentence tokenisation:\", first_article_sentences[:3], sep=\"\\n\")\n",
    "\n",
    "print(\"\\nWord tokenisation:\")\n",
    "# word tokenisation\n",
    "for sentence in first_article_sentences[:3]:\n",
    "    print(word_tokenize(sentence), \"\\n\")\n",
    "\n",
    "first_sentence = first_article_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ee7b77-d95a-490a-b05e-bde36ec93169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfeb6145-22bc-41fb-97d4-8e9e29b053b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords:  ['at', 'to', 'for', 'the', 'to', 'from']\n",
      "Cleaned sentence:  ['Ad', 'sales', 'boost', 'Time', 'Warner', 'profit', 'Quarterly', 'profits', 'US', 'media', 'giant', 'TimeWarner', 'jumped', '76', '%', '$', '1.13bn', '(', 'Â£600m', ')', 'three', 'months', 'December', ',', '$', '639m', 'year-earlier', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stopwords: \", [word for word in word_tokenize(first_sentence) if word.lower() in stopwords_list])\n",
    "print(\"Cleaned sentence: \", [word for word in word_tokenize(first_sentence) if word.lower() not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db76abfe-31cf-4f07-964a-6bd78acc353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
      "\n",
      "Stopwords:  ['at', '%', 'to', '$', '(', ')', 'for', 'the', 'to', ',', 'from', '$', '.']\n",
      "Cleaned sentence:  ['Ad', 'sales', 'boost', 'Time', 'Warner', 'profit', 'Quarterly', 'profits', 'US', 'media', 'giant', 'TimeWarner', 'jumped', '76', '1.13bn', 'Â£600m', 'three', 'months', 'December', '639m', 'year-earlier']\n"
     ]
    }
   ],
   "source": [
    "# Can also remove punctuation by adding to our stopwords list:\n",
    "from string import punctuation\n",
    "print(list(punctuation))\n",
    "stopwords_list = [*stopwords_list, *list(punctuation), *['``', \"''\"]]\n",
    "\n",
    "print(\"\\nStopwords: \", [word for word in word_tokenize(first_sentence) if word.lower() in stopwords_list])\n",
    "print(\"Cleaned sentence: \", [word for word in word_tokenize(first_sentence) if word.lower() not in stopwords_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1bb4e3-772d-4f3d-b6b2-ff2d426b141c",
   "metadata": {},
   "source": [
    "Stemming and Lemmatisation both seek to reduce words to simpler versions, so similar words can be grouped.\n",
    "\n",
    "Stemming works by literally reducing the word\n",
    "* e.g. it might reduce \"jumping\", \"jumps\" and \"jumper\" to just \"jump\".\n",
    "* However, in this case, \"jumper\" is more likely to refer to a type of clothing!\n",
    "\n",
    "Instead, lemmatisation uses grammar rules and dictionary look ups to attempt reduce the words via their meaning.\n",
    "* e.g. it might reduce \"jumping\" and \"jumps\" to \"jump\", and keep \"jumper\" as \"jumper\" - especially if it 'knows' it's a noun!\n",
    "\n",
    "Their are many pre-defined stemming and lemmitasing algorithms. Two of the post common are *Porter Stemmer* and *Wordnet Lemmatizer*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557fb338-6c22-4bff-9998-c9b04d576696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (Porter Stemmer):\n",
      "danc\n",
      "danc\n",
      "danc\n",
      "dancer\n",
      "danc\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "print(\"Stemming (Porter Stemmer):\")\n",
    "for word in [\"dancing\", \"danced\", \"dance\", \"dancer\", \"dances\"]:\n",
    "    print(PorterStemmer().stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2084110-a545-4973-9cc0-e65c6d202cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation (Wordnet Lemmatizer):\n",
      "dancing\n",
      "danced\n",
      "dance\n",
      "dancer\n",
      "dance\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer as WNL\n",
    "print(\"Lemmatisation (Wordnet Lemmatizer):\")\n",
    "for word in [\"dancing\", \"danced\", \"dance\", \"dancer\", \"dances\"]:\n",
    "    print(WNL().lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc357e-2ddc-4178-9fb4-700359c56727",
   "metadata": {},
   "source": [
    "Accurate lemmatisation requires the algorithm to be able to identify grammatical patterns. This process can be improved using Part-of-Speech (POS) tagging, which is when words in the text are labelled, for example, labelling words as adjectives or adverbs.\n",
    "\n",
    "POS tagging can also be done by an algorithm, although different algorithms tag differently. The most common is the Penn Treebank algorithm, which uses labels such as NN (for a noun), RB (adverb), VB (verb), VBD (past-tense verb), etc.\n",
    "\n",
    "However for WordNet Lemmatisation (WNL), we only need to label ADJ (adjectives), VERB (verbs), NOUN (nouns), ADV (adverbs).\n",
    "\n",
    "We can therefore use Penn Treebank POS-tagging, but simplify to the tags used by WNL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb5107cc-f178-41e4-b5f9-aa0c0fda2b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank POS-tags:\n",
      " [('Ad', 'NN'), ('sales', 'NNS'), ('boost', 'VBP'), ('Time', 'NNP'), ('Warner', 'NNP'), ('profit', 'VB'), ('Quarterly', 'JJ'), ('profits', 'NNS'), ('at', 'IN'), ('US', 'NNP'), ('media', 'NNS'), ('giant', 'JJ'), ('TimeWarner', 'NNP'), ('jumped', 'VBD'), ('76', 'CD'), ('%', 'NN'), ('to', 'TO'), ('$', '$'), ('1.13bn', 'CD'), ('(', '('), ('Â£600m', 'NN'), (')', ')'), ('for', 'IN'), ('the', 'DT'), ('three', 'CD'), ('months', 'NNS'), ('to', 'TO'), ('December', 'NNP'), (',', ','), ('from', 'IN'), ('$', '$'), ('639m', 'CD'), ('year-earlier', 'JJ'), ('.', '.')]\n",
      "WNL POS-tags:\n",
      " [('Ad', 'n'), ('sales', 'n'), ('boost', 'v'), ('Time', 'n'), ('Warner', 'n'), ('profit', 'v'), ('Quarterly', 'a'), ('profits', 'n'), ('at', 'n'), ('US', 'n'), ('media', 'n'), ('giant', 'a'), ('TimeWarner', 'n'), ('jumped', 'v'), ('76', 'n'), ('%', 'n'), ('to', 'n'), ('$', 'n'), ('1.13bn', 'n'), ('(', 'n'), ('Â£600m', 'n'), (')', 'n'), ('for', 'n'), ('the', 'n'), ('three', 'n'), ('months', 'n'), ('to', 'n'), ('December', 'n'), (',', 'n'), ('from', 'n'), ('$', 'n'), ('639m', 'n'), ('year-earlier', 'a'), ('.', 'n')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# Assign Penn Treebank tags:\n",
    "print(\"Penn Treebank POS-tags:\\n\", pos_tag(word_tokenize(first_sentence)))\n",
    "\n",
    "# Define a function to convert Penn Treebank POS to WNL tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "WN_pos_tags = []\n",
    "for word, tag in pos_tag(word_tokenize(first_sentence)):\n",
    "    WN_tag = get_wordnet_pos(tag)\n",
    "    WN_pos_tags.append((word, WN_tag))\n",
    "\n",
    "print(\"WNL POS-tags:\\n\", WN_pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416fde4-8888-4d65-a264-cc37c0b3fa6d",
   "metadata": {},
   "source": [
    "We can now perform lemmatisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bff43253-bd60-47e0-b89a-040333df8789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Ad sales boost Time Warner profit Quarterly profits at US media giant TimeWarner jumped 76 % to $ 1.13bn ( Â£600m ) for the three months to December , from $ 639m year-earlier .\n",
      "\n",
      "Default WNL:  ad sale boost time warner profit quarterly profit at u medium giant timewarner jumped 76 % to $ 1.13bn ( â£600m ) for the three month to december , from $ 639m year-earlier .\n",
      "\n",
      "POS-tagged WNL:  ad sale boost time warner profit quarterly profit at u medium giant timewarner jump 76 % to $ 1.13bn ( â£600m ) for the three month to december , from $ 639m year-earlier .\n"
     ]
    }
   ],
   "source": [
    "# Original\n",
    "print(\"Original: \", \" \".join(word_tokenize(first_sentence)))\n",
    "\n",
    "#Without POS tagging:\n",
    "print(\"\\nDefault WNL: \", \" \".join([WNL().lemmatize(word.lower()) for word in word_tokenize(first_sentence)]))\n",
    "\n",
    "# With POS tagging:\n",
    "print(\"\\nPOS-tagged WNL: \", \" \".join([WNL().lemmatize(word.lower(), pos=tag) for word, tag in WN_pos_tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c9d5536-2b42-4f35-a569-d87fb37801d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timewarner', 'say', 'fourth', 'quarter', 'sale', 'rise', '2', '%', 'to', '$', '11.1bn', 'from', '$', '10.9bn', '.']\n"
     ]
    }
   ],
   "source": [
    "# Convert this into a function to take a sentence at a time:\n",
    "def WNL_sentence(input_text):\n",
    "    tagged_text = []\n",
    "    for word, tag in pos_tag(word_tokenize(input_text)):\n",
    "        tagged_text.append((word, get_wordnet_pos(tag)))\n",
    "        \n",
    "    output_text = [WNL().lemmatize(word.lower(), pos=tag) for word, tag in tagged_text]\n",
    "    return(output_text)\n",
    "\n",
    "print(WNL_sentence(first_article_sentences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffb4988c-ead3-4bc4-8d85-716593870a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'perform', 'text', 'analysis', 'bbc', 'news', 'article', 'fun']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can combine all this into one pre-processing function:\n",
    "def preprocess_sentence(input_text):\n",
    "    tagged_text = []\n",
    "    for word, tag in pos_tag(word_tokenize(input_text)):\n",
    "        tagged_text.append((word, get_wordnet_pos(tag)))\n",
    "        \n",
    "    lemma_text = [WNL().lemmatize(word.lower(), pos=tag) for word, tag in tagged_text]\n",
    "    processed_text = [word for word in lemma_text if word not in stopwords_list]\n",
    "    return processed_text\n",
    "\n",
    "preprocess_sentence(\"Today I am performing some text analysis on BBC News articles, which will be fun!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108dbe5-a2f3-492c-b0a0-4e5000ef6b4f",
   "metadata": {},
   "source": [
    "### Perform some basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41232803-4633-46bf-ac10-90ba1f7826cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sentence:  \"Boeing has the latest variant in a very successful line of airplanes and there is no doubt it will continue to be very successful,\" said David Learmount, operations and safety editor at industry magazine Flight International.\n",
      "\n",
      "Processed test_sentence:  ['boeing', 'late', 'variant', 'successful', 'line', 'airplane', 'doubt', 'continue', 'successful', 'say', 'david', 'learmount', 'operation', 'safety', 'editor', 'industry', 'magazine', 'flight', 'international']\n"
     ]
    }
   ],
   "source": [
    "# Process some sentences from our articles:\n",
    "test_article = articles[\"text\"][68]\n",
    "test_sentence = sent_tokenize(test_article)[10]\n",
    "processed_sentence = preprocess_sentence(test_sentence)\n",
    "print(\"test_sentence: \", test_sentence)\n",
    "print(\"\\nProcessed test_sentence: \", processed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df42e131-106b-4bf5-a3f0-011c35af85a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'successful': 2, 'boeing': 1, 'late': 1, 'variant': 1, 'line': 1, 'airplane': 1, 'doubt': 1, 'continue': 1, 'say': 1, 'david': 1, 'learmount': 1, 'operation': 1, 'safety': 1, 'editor': 1, 'industry': 1, 'magazine': 1, 'flight': 1, 'international': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# We can count the occurence of words (into a \"Bag-of-words\"):\n",
    "print(Counter(processed_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c368e888-ef49-49d0-8f19-3e16b975dda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boeing</th>\n",
       "      <th>late</th>\n",
       "      <th>variant</th>\n",
       "      <th>successful</th>\n",
       "      <th>line</th>\n",
       "      <th>airplane</th>\n",
       "      <th>doubt</th>\n",
       "      <th>continue</th>\n",
       "      <th>say</th>\n",
       "      <th>david</th>\n",
       "      <th>learmount</th>\n",
       "      <th>operation</th>\n",
       "      <th>safety</th>\n",
       "      <th>editor</th>\n",
       "      <th>industry</th>\n",
       "      <th>magazine</th>\n",
       "      <th>flight</th>\n",
       "      <th>international</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   boeing  late  variant  successful  line  airplane  doubt  continue  say  \\\n",
       "0       1     1        1           2     1         1      1         1    1   \n",
       "\n",
       "   david  learmount  operation  safety  editor  industry  magazine  flight  \\\n",
       "0      1          1          1       1       1         1         1       1   \n",
       "\n",
       "   international  \n",
       "0              1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And can convert these into a vector:\n",
    "counts_series = pd.Series(Counter(processed_sentence)).to_frame().T\n",
    "counts_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3983f-13b6-42f3-93fd-f31a517e1407",
   "metadata": {},
   "source": [
    "We can also use scikit-learn to vectorise our our test article (note that CountVectorizer requires a text file or a list of strings!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce649e41-f7b8-440b-9087-dadbdddde840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "Count_Vector = CountVectorizer() # initiate default vectoriser\n",
    "Count_Vector.fit_transform([test_sentence])\n",
    "\n",
    "print(Count_Vector.vocabulary_) # NB: the numbers are indices, not counts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529df51-ddde-4a55-84f6-783aabdf0f20",
   "metadata": {},
   "source": [
    "Also note that minimal processing has been done on this string; stop words haven't been removed and the string hasn't been lemmatised.\n",
    "\n",
    "We can pass our custom preprocessing function in the initial settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880290e9-d329-453d-9338-b755f00120e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Count_Vector2 = CountVectorizer(analyzer = preprocess_sentence)\n",
    "vectorised = Count_Vector2.fit_transform([test_sentence, test_sentence2])\n",
    "print(Count_Vector2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e1ddc-67de-400b-b1fb-bf78c3e76e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print these in index order:\n",
    "print(Count_Vector2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3d1e2-f91c-4d1b-80dc-352664e23517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And convert the bag-of-words to a count vector: (one row for each list item)\n",
    "print(vectorised.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55a910-15b1-4103-a222-b25a43a1d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB we can perform this over a whole article using the below:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(article_path) # from the 2nd cell of this notebook\n",
    "\n",
    "with open(article_path, \"r\") as file:\n",
    "    vectoriser = CountVectorizer(analyzer = preprocess_sentence)\n",
    "    vectorised_article = vectoriser.fit_transform(file)\n",
    "print(vectoriser.get_feature_names_out()[:20])\n",
    "print(vectorised_article.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe4370-a6f4-4a44-969b-986124b7fe1e",
   "metadata": {},
   "source": [
    "### Advanced Analysis\n",
    "We can also use ML techniques to analyse our data in more detail!\n",
    "For instance, we can also develop a Naive Bayes that can predict the subject of our articles:\n",
    "\n",
    "*Note we could also tune alpha, but for this example we will keep it simple*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd6dc0-6f61-42fa-9e38-af3a13f3f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f478361-f5c1-4dbb-84ed-6b6caee79d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f93e3-92f9-4569-abde-1b92d67ca03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and training data\n",
    "X = articles[\"text\"]\n",
    "Y = articles[\"category\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "# Setup pipeline:pProcess and vectorise, then classify using multinomial Naive Bayes:\n",
    "NB_model = make_pipeline(CountVectorizer(analyzer = preprocess_sentence), MultinomialNB())\n",
    "\n",
    "# Train the model:\n",
    "NB_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c3ae7-eca4-410c-aa5c-2247dffbd90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "\n",
    "Y_predictions = NB_model.predict(X_test)\n",
    "\n",
    "print(accuracy_score(Y_test, Y_predictions))\n",
    "print(classification_report(Y_test, Y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12c853-ebd9-4ad8-bae9-aa53af858a8f",
   "metadata": {},
   "source": [
    "The model had an accuracy of **96.41%**! We can also visualise these results (and other useful data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2cf044-62ab-4ef0-90ad-d5ec7af079fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Visualise confusion matrix as a heatmap:\n",
    "conf_mat = confusion_matrix(Y_test, Y_predictions, labels=NB_model.classes_)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=False, xticklabels=NB_model.classes_, yticklabels=NB_model.classes_, cmap='Reds')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4e496-9187-46fc-bdec-32b0aab5b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise most important words per category\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = NB_model.named_steps['countvectorizer']\n",
    "classifier = NB_model.named_steps['multinomialnb']\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "class_labels = classifier.classes_\n",
    "\n",
    "top_n = 10  \n",
    "\n",
    "# Create subplots: one for each class\n",
    "fig, axes = plt.subplots(len(class_labels), 1, figsize=(10, 5 * len(class_labels)))\n",
    "\n",
    "for i, class_label in enumerate(class_labels):\n",
    "    # Get top feature indices for this class\n",
    "    class_feature_log_probs = classifier.feature_log_prob_[i]\n",
    "    top_indices = np.argsort(class_feature_log_probs)[-top_n:]\n",
    "    top_words = feature_names[top_indices]\n",
    "    top_probs = class_feature_log_probs[top_indices]\n",
    "    \n",
    "    # Convert log probs to actual probabilities\n",
    "    top_probs_exp = np.exp(top_probs)\n",
    "\n",
    "    # Plot\n",
    "    sns.barplot(x=top_probs_exp, y=top_words, ax=axes[i], palette=\"Blues_d\")\n",
    "    axes[i].set_title(f\"Top {top_n} Words for Category: {class_label}\")\n",
    "    axes[i].set_xlabel(\"P(word | category)\")\n",
    "    axes[i].set_ylabel(\"Word\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be741b18-26ef-4c8b-878c-87a8cae3852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'actual': Y_test,\n",
    "    'predicted': Y_predictions\n",
    "})\n",
    "\n",
    "# Filter misclassified samples\n",
    "misclassified = results_df[results_df['actual'] != results_df['predicted']]\n",
    "\n",
    "print(misclassified.sample(10)[['text', 'actual', 'predicted']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0133a98a-aa43-4581-9de1-71eb3880feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size and style\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=articles, y='category', order=articles['category'].value_counts().index, palette='viridis')\n",
    "\n",
    "plt.title(\"Number of Articles per Category (Whole dataset)\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a115a432-73f8-4cbc-a74a-6fdbd8ef4d19",
   "metadata": {},
   "source": [
    "# Summarising\n",
    "We can also summarise articles, reducing them to shorter formats. Note that special characters need removing for this, but other processing (e.g. lemmatisation) is not required.\n",
    "\n",
    "There are different forms of summarisation, the most common being Extractive, TextRank, and Abstractive.\n",
    "\n",
    "**Extractive** does not generate new text, and simply select and combines existing sentences based on rules. Sentence importance is estimated based on rules, such as the presence of keywords or named entities, position in the text (early and later sentences are more important), and the freqency of non-stop words. It preserves the original content of the selected sentences best, and is therefore less prone to hallucinating/creating errors. It is also quick and inexpensive.\n",
    "\n",
    "**TextRank** is a form of extractive summarisation, where connections are graphed between similar sentences, with highly-connected sentences being selected as most important. It also doesn't create new text, and can be sophisticated than tradional extration. However, it is more costly to run.\n",
    "\n",
    "**Abstractive** summarisation aims to \"understand\" the original text and rephrase by creating new sentences. It uses deep learning (seq2seq) models. Text is encoded, and then decoded into a summary. It offers greater level of reduction than extractive methods, however it is much more computationally expensive, and as it is generative it can hallucinate and create errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "870a2b7e-c2bd-4d7e-a0c9-24b8890011c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a reminder of our first_article:\n",
    "first_article[:200]\n",
    "\n",
    "# remove special characters:\n",
    "text = first_article.replace('\\n', ' ').replace('\\r', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5b41653-a665-4e46-9eb2-81de4d07e44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article:\n",
      " Ad sales boost Time Warner profit  Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.  The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.  Time  \n",
      "...\n",
      "\n",
      "Traditional Extractive Summary:\n",
      "- Ad sales boost Time Warner profit  Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n",
      "- TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.\n",
      "- However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.\n"
     ]
    }
   ],
   "source": [
    "#Extractive (Traditional) Summarisation:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Split into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Rank sentences by their \"importance\" (sum of similarities)\n",
    "sentence_scores = similarity_matrix.sum(axis=1)\n",
    "\n",
    "# Select top N sentences\n",
    "top_n = 3\n",
    "top_sentence_indices = sentence_scores.argsort()[-top_n:][::-1]\n",
    "top_sentences = [sentences[i] for i in sorted(top_sentence_indices)]\n",
    "\n",
    "# Print results\n",
    "print(\"Original Article:\\n\", text[:500], \"\\n...\")\n",
    "print(\"\\nTraditional Extractive Summary:\")\n",
    "for sent in top_sentences:\n",
    "    print(\"-\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20c61be6-6a5d-4a23-9568-7c1c2e10c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRank Summary:\n",
      "- It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband.\n",
      "- But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results.\n",
      "- It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue.\n"
     ]
    }
   ],
   "source": [
    "# TextRank Summarisation:\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "# Set up parser and summarizer\n",
    "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "summarizer = TextRankSummarizer()\n",
    "\n",
    "# Generate summary with N sentences\n",
    "summary = summarizer(parser.document, sentences_count=3)\n",
    "\n",
    "# Print result\n",
    "print(\"TextRank Summary:\")\n",
    "for sentence in summary:\n",
    "    print(\"-\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "047720b4-487d-4b8b-906d-869256391919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Summary:\n",
      " TimeWarner profits up 76% to $1.13bn for the three months to December. Firm is now one of the biggest investors in Google.\n"
     ]
    }
   ],
   "source": [
    "# Abstractive Summarisation:\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary (you can tweak length and decoding strategy)\n",
    "summary_ids = model.generate(\n",
    "    inputs,\n",
    "    max_length=130,\n",
    "    min_length=30,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode and print the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"BART Summary:\\n\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
